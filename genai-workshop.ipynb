{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "I8F3XGz_dyXc",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Neo4j Generative AI Workshop\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xpilasneo4j/genai-workshop-2024/blob/main/genai-workshop.ipynb)\n",
    "\n",
    "In this workshop, you will learn how to use Neo4j Knowledge Graphs to make Large Language Models (LLMs) useful for more real-world use cases.\n",
    "\n",
    "We walk through an example that uses real-world customer and product data from a fashion, style, and beauty retailer. We show how you can use a knowledge graph to ground an LLM, enabling it to build tailored marketing content personalized to each customer based on their interests and shared purchase histories. We use a pattern called Retrieval-Augmented Generation (RAG) to accomplish this.  Specifically, one that leverages not only vector search but also graph pattern matching and graph machine learning to provide more relevant personalized results to customers.\n",
    "\n",
    "This notebook walks through the end-to-end process, including:\n",
    "- Building the knowledge graph\n",
    "- Vector search & text embedding\n",
    "- Using graph patterns in Cypher to improve semantic search with context\n",
    "- Further augmenting semantic search with knowledge graph inference & ML\n",
    "- Building the LLM chain and demo app for generating content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ Graph Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cmjr1dz8dyXd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8yxD7Ah0ZACB",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Some Logics\n",
    "1. Make a copy of this notebook in Colab by [clicking here](https://colab.research.google.com/github/xpilasneo4j/genai-workshop-2024/blob/main/genai-workshop.ipynb).\n",
    "2. Run the pip install below to get the necessary dependencies.  this can take a while. Then run the following cell to import relevant libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY1XylsiZACB",
    "pycharm": {
     "name": "#%%capture\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install sentence_transformers langchain langchain-openai langchain-community openai tiktoken python-dotenv gradio graphdatascience altair neo4j_tools\n",
    "%pip install \"vegafusion[embed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7psF1otOdyXe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from graphdatascience import GraphDataScience\n",
    "from neo4j_tools import gds_db_load, gds_utils\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "import gradio as gr\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.width', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_ar1ZFhPdyXe",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Setup Credentials and Environment Variables\n",
    "\n",
    "There are two things you need here.\n",
    "1. Start a *Blank Sandbox - Graph Data Science* [Neo4j Sandbox](https://sandbox.neo4j.com/). \n",
    "<p></p>\n",
    "<img src=\"img/sandbox.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get your URI and password and plug them in below.  Do not change the Neo4j username.\n",
    "<p></p>\n",
    "<img src=\"img/sandbox-detail.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get your OpenAI API key.  You can use [this one](https://docs.google.com/document/d/19Lqjd0MqRs088KUVnd23ZrVU9G0OAg-53U72VrFwwms/edit) if you do not have one already\n",
    "\n",
    "To make this easy, you can write the credentials and env variables directly into the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQ9s0ZWhekd8"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Neo4j\n",
    "NEO4J_URI = 'bolt://34.202.229.218:7687' #change this\n",
    "NEO4J_PASSWORD = 'terminologies-fire-planet' #change this\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "AURA_DS = False\n",
    "\n",
    "# AI\n",
    "LLM = 'gpt-4'\n",
    "\n",
    "# OpenAI - Required when using OpenAI models\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-...' #change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-98NuINdyXe"
   },
   "outputs": [],
   "source": [
    "# You can skip this cell if not using a ws.env file - alternative to above\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "if os.path.exists('ws.env'):\n",
    "    load_dotenv('ws.env', override=True)\n",
    "\n",
    "    # Neo4j\n",
    "    NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "    NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "    NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "    AURA_DS = eval(os.getenv('AURA_DS').title())\n",
    "\n",
    "    # AI\n",
    "    LLM = os.getenv('LLM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "67Tm1p3LdyXe",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Knowledge Graph Building\n",
    "\n",
    "<img src=\"img/hm-banner.png\" alt=\"summary\" width=\"2000\"/>\n",
    "\n",
    "We begin by building our knowledge graph. This workshop will leverage the [H&M Personalized Fashion Recommendations Dataset](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data), a sample of real customer purchase data that includes rich information around products including names, types, descriptions, department sections, etc.\n",
    "\n",
    "Below is the graph data model we will use:\n",
    "\n",
    "<img src=\"img/data-model.png\" alt=\"summary\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "yU13AU73dyXf",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Connect to Neo4j\n",
    "\n",
    "We will use the [Graph Data Science Python Client](https://neo4j.com/docs/graph-data-science-client/current/) to connect to Neo4j. This client makes it convenient to display results, as we will see later.  Perhaps more importantly, it allows us to easily run [Graph Data Science](https://neo4j.com/docs/graph-data-science/current/introduction/) algorithms from Python.\n",
    "\n",
    "This client will only work if your Neo4j instance has Graph Data Science installed.  If not, you can still use the [Neo4j Python Driver](https://neo4j.com/docs/python-manual/current/) or use Langchainâ€™s Neo4j Graph object that we will see later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92GFeMaRdyXf"
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "# Use Neo4j URI and credentials according to our setup\n",
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
    "    aura_ds=AURA_DS)\n",
    "\n",
    "# Necessary if you enabled Arrow on the db - this is true for AuraDS\n",
    "gds.set_database(\"neo4j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "mO32Q0Thw5JO",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Test your connection by running the below.  It should output your system information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "QJ6qg0qMw5JO",
    "outputId": "378854bf-e03c-4e62-f89c-589c68d8955f"
   },
   "outputs": [],
   "source": [
    "gds.debug.sysInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2H3Yb5JLdyXf",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Get Data, Create Constraints, and Load\n",
    "\n",
    "Before loading data into Neo4j, it is usually best practice to create Key or Uniqueness constraints for nodes. These [constraints](https://neo4j.com/docs/cypher-manual/current/constraints/) act as an index with some validation on unique id properties and thus make `MATCH` statements run significantly faster. Not doing this can result in a VERY slow ingest, so this is a critical step.\n",
    "\n",
    "We will be using convenience functions for loading nodes and relationships in batch. As the data load runs, you can see the Cypher statements being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vuweMfqpdyXf",
    "outputId": "449362a2-f73f-423c-bfc3-4883d7cd4659"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from neo4j_tools import gds_db_load\n",
    "\n",
    "# get source data - it has been pre-formatted. If you would like to re-generate from source on Kaggle, see the data-prep.ipynb notebook\n",
    "department_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/department.csv')\n",
    "product_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/product.csv')\n",
    "article_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/article.csv')\n",
    "customer_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/customer.csv')\n",
    "transaction_df = pd.read_csv('https://storage.googleapis.com/neo4j-workshop-data/genai-hm/transaction.csv')\n",
    "\n",
    "# create constraints - one uniqueness constraint for each node label\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_department_no IF NOT EXISTS FOR (n:Department) REQUIRE n.departmentNo IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_product_code IF NOT EXISTS FOR (n:Product) REQUIRE n.productCode IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_article_id IF NOT EXISTS FOR (n:Article) REQUIRE n.articleId IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_customer_id IF NOT EXISTS FOR (n:Customer) REQUIRE n.customerId IS UNIQUE')\n",
    "\n",
    "# load nodes\n",
    "gds_db_load.load_nodes(gds, department_df, 'departmentNo', 'Department')\n",
    "gds_db_load.load_nodes(gds, article_df.drop(columns=['productCode', 'departmentNo']), 'articleId', 'Article')\n",
    "gds_db_load.load_nodes(gds, product_df, 'productCode', 'Product')\n",
    "gds_db_load.load_nodes(gds, customer_df, 'customerId', 'Customer')\n",
    "\n",
    "# load relationships\n",
    "gds_db_load.load_rels(gds, article_df[['articleId', 'departmentNo']], source_target_labels=('Article', 'Department'),\n",
    "                      source_node_key='articleId', target_node_key='departmentNo',\n",
    "                      rel_type='FROM_DEPARTMENT')\n",
    "gds_db_load.load_rels(gds, article_df[['articleId', 'productCode']], source_target_labels=('Article', 'Product'),\n",
    "                      source_node_key='articleId',target_node_key='productCode',\n",
    "                      rel_type='VARIANT_OF')\n",
    "gds_db_load.load_rels(gds, transaction_df, source_target_labels=('Customer', 'Article'),\n",
    "                      source_node_key='customerId', target_node_key='articleId', rel_key='txId',\n",
    "                      rel_type='PURCHASED')\n",
    "\n",
    "# convert transaction dates\n",
    "gds.run_cypher('''\n",
    "MATCH (:Customer)-[r:PURCHASED]->()\n",
    "SET r.tDat = date(r.tDat)\n",
    "''')\n",
    "\n",
    "# create combined text property. This will help simplify later with semantic search and RAG\n",
    "gds.run_cypher(\"\"\"\n",
    "    MATCH(p:Product)\n",
    "    SET p.text = '##Product\\n' +\n",
    "        'Name: ' + p.prodName + '\\n' +\n",
    "        'Type: ' + p.productTypeName + '\\n' +\n",
    "        'Group: ' + p.productGroupName + '\\n' +\n",
    "        'Garment Type: ' + p.garmentGroupName + '\\n' +\n",
    "        'Description: ' + p.detailDesc\n",
    "    RETURN count(p) AS propertySetCount\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SpKcGgRZdyXg",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2/ Vector Search\n",
    "\n",
    "In this section, we will build text embeddings out of product descriptions and demonstrate how to leverage the Neo4j vector index for vector search. We will also introduce the use of [LangChain](https://www.langchain.com/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "-WrlFCN1dyXg",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Creating Text Embeddings\n",
    "\n",
    "To start we need to make embeddings for our product nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zuqGNOqTZACF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First, we will instantiate our embedding model. This notebook has just been tested with OpenAI, but these models are pluggable. You could choose embedding models from other providers, including cloud providers like AWS Bedrock and Vertex AI Generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwRdCBawdyXg"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "embedding_dimension = 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FzzEFpJPZACF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now let's create a dataframe with a text column to embed.  In this case, we will combine multiple text columns, such as product name, type, description, etc.  This provides the embedding model with more context.  Some products are missing a description (a small minority).  For our intents and purposes we will leave them out. In a more in-depth workflow, you would likely want to impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "36VCtp9HdyXh",
    "outputId": "81d33bc8-c646-49f6-aa85-5eaace2434b0"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.width', 0)\n",
    "\n",
    "product_emb_df = product_df[['productCode', 'prodName', 'productTypeName', 'productGroupName', 'garmentGroupName', 'detailDesc']]\n",
    "product_emb_df = product_emb_df[product_emb_df.detailDesc.notnull()]\n",
    "\n",
    "def create_doc(row):\n",
    "    return f'''\n",
    "##Product\n",
    "Name: {row.prodName}\n",
    "Type: {row.productTypeName}\n",
    "Group: {row.productGroupName}\n",
    "Garment Type: {row.garmentGroupName}\n",
    "Description: {row.detailDesc}\n",
    "'''\n",
    "\n",
    "product_emb_df['text'] = product_emb_df.apply(create_doc, axis=1)\n",
    "product_emb_df = product_emb_df.drop(columns=['prodName', 'productTypeName', 'productGroupName', 'garmentGroupName', 'detailDesc'])\n",
    "product_emb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bLpMaWmkZACF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now letâ€™s embed the text with OpenAI.  We will chunk this into batches for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5qUudN0KdyXh",
    "outputId": "1da33635-e206-4f14-bbb9-8b88120434f0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "count = 0\n",
    "embeddings = []\n",
    "for docs in gds_db_load.chunks(product_emb_df.text, n=500):\n",
    "    count += len(docs)\n",
    "    print(f'Embedded {count} of {product_emb_df.shape[0]}')\n",
    "    embeddings.extend(embedding_model.embed_documents(docs))\n",
    "\n",
    "# Set as column of dataframe to prepare for loading\n",
    "product_emb_df['textEmbedding'] = embeddings\n",
    "product_emb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "zOudn8tmdyXo",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Create Vector Properties and Index\n",
    "\n",
    "Now, we will load the embeddings into Neo4j by MATCHing on ProductCode, then calling the `db.create.setNodeVectorProperty` to set the embedding property. This special function is used to set the properties as floats rather than double precision, which requires more space.  This becomes important as these embedding vectors tend to be long, and the size can add up quickly.\n",
    "\n",
    "After bulk loading, we will create the vector index. The [Neo4j Vector Index](https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/) enables efficient Approximate Nearest Neighbor (ANN) search with vectors. It uses the Hierarchical Navigable Small World (HNSW) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yWtG_wWKdyXo",
    "outputId": "a76a919d-f329-454e-cdc9-92a35fc2cf18"
   },
   "outputs": [],
   "source": [
    "# load vector properties\n",
    "records = product_emb_df[['productCode', 'textEmbedding']].to_dict('records')\n",
    "print(f'======  loading Product text embeddings ======')\n",
    "total = len(records)\n",
    "print(f'staging {total:,} records')\n",
    "cumulative_count = 0\n",
    "for recs in gds_db_load.chunks(records, n=100):\n",
    "    res = gds.run_cypher('''\n",
    "    UNWIND $recs AS rec\n",
    "    MATCH(n:Product {productCode: rec.productCode})\n",
    "    CALL db.create.setNodeVectorProperty(n, \"textEmbedding\", rec.textEmbedding)\n",
    "    RETURN count(n) AS propertySetCount\n",
    "    ''', params={'recs': recs})\n",
    "    cumulative_count += res.iloc[0, 0]\n",
    "    print(f'Set {cumulative_count:,} of {total:,} text embeddings')\n",
    "\n",
    "#create index\n",
    "gds.run_cypher('''\n",
    "CREATE VECTOR INDEX product_text_embeddings IF NOT EXISTS FOR (n:Product) ON (n.textEmbedding)\n",
    "OPTIONS {indexConfig: {\n",
    " `vector.dimensions`: toInteger($dim),\n",
    " `vector.similarity_function`: 'cosine'\n",
    "}}''', params={'dim': embedding_dimension})\n",
    "\n",
    "gds.run_cypher('CALL db.awaitIndex(\"product_text_embeddings\", 300)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rGoMu3bqekeE",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Vector Search Using Cypher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SN7zU0GbZACF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "To do vector search, we need to:\n",
    "1. Take the search prompt and convert it to an embedding query vector\n",
    "2. Use similarity search with that new vector to pull semantically similar documents\n",
    "\n",
    "Below is an example of converting a search prompt into a query vector. We use our same embedding model to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elNIp0_BdyXo",
    "outputId": "24ec55bf-c8d0-4c27-83e2-75acce912e00"
   },
   "outputs": [],
   "source": [
    "#search_prompt = 'denim jeans, loose fit, high-waist'\n",
    "search_prompt = 'Oversized Sweaters'\n",
    "\n",
    "query_vector = embedding_model.embed_query(search_prompt)\n",
    "print(f'query vector length: {len(query_vector)}')\n",
    "print(f'query vector sample: {query_vector[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7yJwN1fxZACG",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we can take that and use it in a Cypher query with the vector index to retrieve semantically similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "pRTY5LnPdyXp",
    "outputId": "15725432-f1d9-468e-9419-ade66c0ac9d8"
   },
   "outputs": [],
   "source": [
    "gds.run_cypher('''\n",
    "CALL db.index.vector.queryNodes(\"product_text_embeddings\", 10, $queryVector)\n",
    "YIELD node AS product, score\n",
    "RETURN product.productCode AS productCode,\n",
    "    product.text AS text,\n",
    "    score\n",
    "''', params={'queryVector': query_vector})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLmyZ_5lhTRm"
   },
   "source": [
    "### Vector Search Using Langchain\n",
    "\n",
    "We can also do this vector search with Langchain, a recommended approach going forward.  To do this, we use the Neo4jVector class and call the below method to set up from an existing index in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofoi5aJvekeF"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "\n",
    "kg_vector_search = Neo4jVector.from_existing_index(\n",
    "    embedding=embedding_model,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name='product_text_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cz1VqGy9ZACG",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Langchain can handle embedding the query vector and retrieving from Neo4j behind the scenes, making our lives easier.  Langchain uses a similar query as above and retrieves the `text` property we set for each Product node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDFoMXmbekeF",
    "outputId": "ad51a5c6-4402-4f3c-e2d9-3b3354569f6e"
   },
   "outputs": [],
   "source": [
    "res = kg_vector_search.similarity_search(search_prompt, k=10)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "id": "qQP7e2dcekeF",
    "outputId": "610d663e-15a6-4a84-c16d-b8fe16f8283c"
   },
   "outputs": [],
   "source": [
    "# Visualize as a dataframe\n",
    "pd.DataFrame([{'document': d.page_content} for d in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "IXn_6bFCekeF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Try Yourself\n",
    "\n",
    "Experiment with your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHayKAOlekeF"
   },
   "outputs": [],
   "source": [
    "res = kg_vector_search.similarity_search('type your prompt here!', k=10)\n",
    "pd.DataFrame([{'document': d.page_content} for d in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tofjZIsSekeF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3/ Semantic Search with Context (Personalizations)\n",
    "__Using Graph Patterns to Improve Context in Search & Retrieval__\n",
    "\n",
    "Above, we saw how you can use the vector index to find semantic similar products in user searches.  This is an extremely powerful tool; however, it is not the end-all be-all.  It doesn't consider much of the customer data and isn't very personalized. Furthermore, some search\n",
    "prompts, like \"Oversized Sweater,\" are very general and can match a large number of products, many of which won't be relevant to the specific user conducting the search.\n",
    "\n",
    "We have a rich knowledge graph full of customer information; let's see how to leverage it to improve search experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0NrRQaY3ZACH",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Explore in Sandbox\n",
    "To understand how to better leverage our graph, let's explore in Neo4j Browser on our sandbox instance.\n",
    "\n",
    "#### Exploring the Graph\n",
    "First, let's validate the schema by calling the below\n",
    "```\n",
    "CALL db.schema.visualization()\n",
    "```\n",
    "\n",
    "We can also use Cypher to sample the graph. Run the below query in Browser and explore the results:\n",
    "```\n",
    "MATCH (p:Product)<-[v:VARIANT_OF]-(a:Article)<-[t:PURCHASED]-(c:Customer)\n",
    "RETURN * LIMIT 150\n",
    "```\n",
    "\n",
    "You should get something that looks like the below.  Notice the multi-hop connections between customers based on purchases. This is valuable information encoded in our graph!\n",
    "\n",
    "<img src=\"img/sample-query.png\" alt=\"summary\" width=\"1000\"/>\n",
    "\n",
    "#### Understanding Shared Customer Behavior\n",
    "\n",
    "Now let's consider a single customer's purchase history.  We will choose the below customer by setting customerId as a parameter.\n",
    "\n",
    "```\n",
    ":params {customerId:'daae10780ecd14990ea190a1e9917da33fe96cd8cfa5e80b67b4600171aa77e0'}\n",
    "```\n",
    "\n",
    "Then we can run the below Cypher to pull history:\n",
    "\n",
    "```\n",
    "MATCH(c:Customer {customerId: $customerId})-[t:PURCHASED]->(:Article)\n",
    "-[:VARIANT_OF]->(p:Product)\n",
    "RETURN p.productCode AS productCode,\n",
    "    p.prodName AS prodName,\n",
    "    p.productTypeName AS productTypeName,\n",
    "    p.garmentGroupName AS garmentGroupName,\n",
    "    p.detailDesc AS detailDesc,\n",
    "    t.tDat AS purchaseDate\n",
    "ORDER BY t.tDat DESC\n",
    "```\n",
    "Expected results:\n",
    "<img src=\"img/purchase-history.png\" alt=\"summary\" width=\"1000\"/>\n",
    "\n",
    "These purchases are ordered by transaction date. The most recent purchases should be the \"Tove Top\" and the \"Rosemary Dress\".\n",
    "\n",
    "Now let's consider just the latest products in the above list and see what else we could recommend to customers who liked them.  The following Cypher query provides potential answers by finding the most popular products among customers who purchased these.\n",
    "\n",
    "```\n",
    "//pull the latest purchases\n",
    "MATCH(c:Customer {customerId: $customerId})-[t:PURCHASED]->()\n",
    "WITH max(t.tDat) AS latestPurchases\n",
    "//find related products based on customer purchases\n",
    "MATCH(c:Customer {customerId: $customerId})-[:PURCHASED {tDat: latestPurchases}]->(:Article)<-[:PURCHASED]-(:Customer)-[:PURCHASED]->(:Article)\n",
    "    -[:VARIANT_OF]->(p:Product)\n",
    "RETURN p.productCode AS productCode,\n",
    "    p.prodName AS prodName,\n",
    "    p.productTypeName AS productTypeName,\n",
    "    p.garmentGroupName AS garmentGroupName,\n",
    "    count(*) AS commonPurchaseScore,\n",
    "    p.detailDesc AS detailDesc\n",
    "ORDER BY commonPurchaseScore DESC\n",
    "```\n",
    "\n",
    "Expected results:\n",
    "<img src=\"img/related-products.png\" alt=\"summary\" width=\"1000\"/>\n",
    "\n",
    "__You will see that some of the above results seem intuitive...but not all of them right away...and that is exactly the point!\n",
    "There is information encoded inside the knowledge graph about customer preferences that isn't inferable from the product text documents.__\n",
    "\n",
    "__This is one example of where enterprise-specific data, expressed as structured relationships, contains critical information that is impossible to find elsewhere. This is why, for many real-world applications, you should consider backing semantic search and GenAI with Knowledge Graphs.__\n",
    "\n",
    "Now letâ€™s see how to apply this pattern in our semantic search and retrieval!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "QdiPqIq2dyXp",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Personalizing Results Based on Customer Behavior in the Graph\n",
    "\n",
    "As we saw in Browser, an important piece of information expressed in this graph, but not directly in the product documents and text embeddings, is customer purchasing behavior.  We saw that we can use graph patterns in Cypher to extract insights from these. Now that we know how this pattern works, we can apply it to our semantic search to make results more personalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "L0ySzeloZACI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "To do this, we append a MATCH statement to the end of our initial vector search query.  Basically, once the product documents are returned, we can re-calculate how they would score according to the query above and use that to re-rank the search results.\n",
    "\n",
    "Langchain makes this easy by allowing for a `retrieval_query` argument where we can put in the pattern we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eKwXKf_ekeF"
   },
   "outputs": [],
   "source": [
    "CUSTOMER_ID = \"daae10780ecd14990ea190a1e9917da33fe96cd8cfa5e80b67b4600171aa77e0\"\n",
    "\n",
    "kg_personalized_search = Neo4jVector.from_existing_index(\n",
    "    embedding=embedding_model,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name='product_text_embeddings',\n",
    "    retrieval_query=f\"\"\"\n",
    "    WITH node AS product, score AS searchScore\n",
    "\n",
    "    OPTIONAL MATCH(product)<-[:VARIANT_OF]-(:Article)<-[:PURCHASED]-(:Customer)\n",
    "    -[:PURCHASED]->(a:Article)<-[:PURCHASED]-(:Customer {{customerId: '{CUSTOMER_ID}'}})\n",
    "\n",
    "    WITH count(a) AS purchaseScore, product.text AS text, searchScore, product.productCode AS productCode\n",
    "    RETURN text,\n",
    "        (1+purchaseScore)*searchScore AS score,\n",
    "        {{productCode: productCode, purchaseScore:purchaseScore, searchScore:searchScore}} AS metadata\n",
    "    ORDER BY purchaseScore DESC, searchScore DESC LIMIT 15\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9BqGWliZZACI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now let's run it to see if/how our results have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 909
    },
    "id": "rUCSQyrZekeF",
    "outputId": "f15d17d1-2fbd-47db-bbd4-b9058e127924"
   },
   "outputs": [],
   "source": [
    "res = kg_personalized_search.similarity_search(search_prompt, k=100)\n",
    "\n",
    "# Visualize as a dataframe\n",
    "pd.DataFrame([{'productCode': d.metadata['productCode'],\n",
    "               'document': d.page_content,\n",
    "               'searchScore': d.metadata['searchScore'],\n",
    "               'purchaseScore': d.metadata['purchaseScore']} for d in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "t55maEsfdyXp",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4/ Graph Data Science & ML (Recommendations)\n",
    "\n",
    "We saw above how to use graph pattern matching to personalize semantic search and make it more contextually relevant.\n",
    "\n",
    "In addition to this, we also have [Graph Data Science algorithms and machine learning](https://neo4j.com/docs/graph-data-science/current/introduction/) which allows you to enrich your knowledge graph with additional properties, relationships, and graph metrics. These can in-turn be leveraged in search and retrieval to improve and augment results.\n",
    "\n",
    "We will walk through an example of this below, where we use Graph Data Science to augment retrieval with additional product recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "vec2fJG7ZACI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Below, we use graph machine learning to create relationships that can help us make personalized recommendations based on purchase history.\n",
    "\n",
    "\n",
    "We do this by leveraging Node Embeddings with K-Nearest Neighbor (KNN). Specifically we:\n",
    "1. Use Node embeddings to encode the similarity between articles based on shared customer search history\n",
    "2. Take those node embeddings as input to KNN, an unsupervised learning technique, to link the most similar products together with a \"CUSTOMERS_ALSO_LIKE\" relationship.\n",
    "3. We can then use Cypher patterns at query time to grab recommended items based on a customer's recent purchase history.\n",
    "This process helps scale memory-based recommendation techniques to larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtGT6HfgekeG",
    "outputId": "5dc1926a-10ae-428c-9c94-40dd8185d439"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from neo4j_tools import gds_utils\n",
    "\n",
    "#clear past GDS analysis in the case of re-running\n",
    "gds_utils.clear_all_gds_graphs(gds)\n",
    "gds_utils.delete_relationships('CUSTOMERS_ALSO_LIKE', gds, src_node_label='Article')\n",
    "\n",
    "\n",
    "# graph projection - project co-purchase graph into analytics workspace\n",
    "gds.run_cypher('''\n",
    "   MATCH (a1:Article)<-[:PURCHASED]-(:Customer)-[:PURCHASED]->(a2:Article)\n",
    "   WITH gds.graph.project(\"proj\", a1, a2,\n",
    "       {sourceNodeLabels: labels(a1),\n",
    "       targetNodeLabels: labels(a2),\n",
    "       relationshipType: \"COPURCHASE\"}) AS g\n",
    "   RETURN g.graphName\n",
    "   ''')\n",
    "g = gds.graph.get(\"proj\")\n",
    "\n",
    "# create FastRP node embeddings\n",
    "gds.fastRP.mutate(g, mutateProperty='embedding', embeddingDimension=128, randomSeed=7474, concurrency=4, iterationWeights=[0.0, 1.0, 1.0])\n",
    "\n",
    "# draw KNN\n",
    "knn_stats = gds.knn.write(g, nodeProperties=['embedding'], nodeLabels=['Article'],\n",
    "                  writeRelationshipType='CUSTOMERS_ALSO_LIKE', writeProperty='score',\n",
    "                  sampleRate=1.0, initialSampler='randomWalk', concurrency=1, similarityCutoff=0.75, randomSeed=7474)\n",
    "\n",
    "# write embeddings back to database to introspect later\n",
    "gds.graph.writeNodeProperties(g, ['embedding'], ['Article'])\n",
    "\n",
    "# clear graph projection once done\n",
    "g.drop()\n",
    "\n",
    "# output knn stats\n",
    "knn_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Zu6MWuaDekeG",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Visualize Node Embeddings\n",
    "To better help understand what the Node embeddings are doing, letâ€™s pull some back and visualize them!\n",
    "\n",
    "__NOTE__: The visualization below should be pre-rendered to cut down on runtime. Running the TSNE cell can take ~5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "35pfsqVwdyXq",
    "outputId": "e34eb6f5-d5ed-4310-ec6e-95ff9a1428f0"
   },
   "outputs": [],
   "source": [
    "graph_emb_df = gds.run_cypher('''\n",
    "MATCH (p:Product)<-[:VARIANT_OF]-(a:Article)-[:FROM_DEPARTMENT]-(d)\n",
    "RETURN a.articleId AS articleId,\n",
    "    p.prodName AS productName,\n",
    "    p.productTypeName AS productTypeName,\n",
    "    d.departmentName AS departmentName,\n",
    "    d.sectionName AS sectionName,\n",
    "    p.detailDesc AS detailDesc,\n",
    "    a.embedding AS embedding\n",
    "''')\n",
    "\n",
    "#view a sample\n",
    "graph_emb_df.loc[:3, ['articleId', 'embedding']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNXMIDaGekeG"
   },
   "outputs": [],
   "source": [
    "# Skip this for Colab Workshop\n",
    "# import numpy as np\n",
    "# from sklearn.manifold import TSNE\n",
    "#\n",
    "# df = graph_emb_df.copy()\n",
    "# filtered_node_df = df[df.embedding.apply(lambda x: np.count_nonzero(x) > 0)].reset_index(drop=True)\n",
    "# # instantiate the TSNE model\n",
    "# tsne = TSNE(n_components=2, random_state=7474, init='random', learning_rate=\"auto\")\n",
    "# # Use the TSNE model to fit and output a 2-d representation\n",
    "# E = tsne.fit_transform(np.stack(filtered_node_df['embedding'], axis=0))\n",
    "#\n",
    "# coord_df = pd.concat([filtered_node_df, pd.DataFrame(E, columns=['x', 'y'])], axis=1)\n",
    "# coord_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 794
    },
    "id": "7yNBed6eekeG",
    "outputId": "e98403e3-3394-42df-abed-b98e4b29f539"
   },
   "outputs": [],
   "source": [
    "# Skip this for Colab Workshop\n",
    "# import altair as alt\n",
    "# from sklearn.manifold import TSNE\n",
    "#\n",
    "# alt.data_transformers.disable_max_rows()\n",
    "# chart = alt.Chart(coord_df.sample(n=5000, random_state=7474)).mark_circle(size=60).encode(\n",
    "#     x='x',\n",
    "#     y='y',\n",
    "#     tooltip=['productName', 'productTypeName', 'departmentName' , 'sectionName', 'detailDesc']\n",
    "# ).properties(title=\"Article Embedding (2D Representation)\", width=750, height=700)\n",
    "#\n",
    "# chart = chart.configure_axis(titleFontSize=20)\n",
    "# chart.configure_legend(labelFontSize = 20)\n",
    "# chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7nNobdJhekeH",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Personalized Recommendations\n",
    "\n",
    "Now, let's construct a KG store to retrieve recommendations for a user.  This need not be based on a user prompt or vector search. Instead, we will make it purely based on purchase history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NVy9J6T5ekeH",
    "outputId": "ec61f291-e38e-4836-966a-b0d51f7a5116"
   },
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "kg = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD)\n",
    "\n",
    "res = kg.query('''\n",
    "    MATCH(:Customer {customerId:$customerId})-[:PURCHASED]->(:Article)\n",
    "    -[r:CUSTOMERS_ALSO_LIKE]->(:Article)-[:VARIANT_OF]->(product)\n",
    "    RETURN product.productCode AS productCode,\n",
    "        product.prodName AS prodName,\n",
    "        product.productTypeName AS productType,\n",
    "        product.text AS document,\n",
    "        sum(r.score) AS recommenderScore\n",
    "    ORDER BY recommenderScore DESC LIMIT $k\n",
    "    ''', params={'customerId': CUSTOMER_ID, 'k':15})\n",
    "\n",
    "#visualize as dataframe. result is list of dict\n",
    "pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "90E9HGu4dyXq",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 5/ LLM Powered Content Generator\n",
    "\n",
    "Let's use an LLM to automatically generate content for targeted marketing campaigns grounded with our knowledge graph using the above tools.\n",
    "Here is a quick example for generating promotional messages, but you can create all sorts of content with this!\n",
    "\n",
    "For our first message, let's consider a scenario where a user recently searched for products, but perhaps didn't commit to a purchase yet. We now want to send a message to promote relevant products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JI9LVEdKekeH"
   },
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "#Instantiate LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=LLM, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8I6JesV0ekeH",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Create Knowledge Graph Stores for Retrieval\n",
    "\n",
    "To ground our content generation, we need to define retrievers to pull information from our knowledge graph.  Let's make two stores:\n",
    "1. Personalized Search Retriever (`kg_personalized_search`): Based on recent customer searches and purchase history, pull relevant products.\n",
    "2. Recommendations retriever (`kg_recommendations_app`): Based on our Graph ML, what else can we recommend to them to pair with the relevant products?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLBBVRXwdyXq"
   },
   "outputs": [],
   "source": [
    "# This will be a function so we can change per customer id\n",
    "# We will use a mock URL for our sources in the metadata\n",
    "def kg_personalized_search_gen(customer_id):\n",
    "    return Neo4jVector.from_existing_index(\n",
    "        embedding=embedding_model,\n",
    "        url=NEO4J_URI,\n",
    "        username=NEO4J_USERNAME,\n",
    "        password=NEO4J_PASSWORD,\n",
    "        index_name='product_text_embeddings',\n",
    "        retrieval_query=f\"\"\"\n",
    "        WITH node AS product, score AS searchScore\n",
    "\n",
    "        OPTIONAL MATCH(product)<-[:VARIANT_OF]-(:Article)<-[:PURCHASED]-(:Customer)\n",
    "        -[:PURCHASED]->(a:Article)<-[:PURCHASED]-(:Customer {{customerId: '{customer_id}'}})\n",
    "        WITH count(a) AS purchaseScore, product, searchScore\n",
    "        RETURN product.text + '\\nurl: ' + 'https://representative-domain/product/' + product.productCode  AS text,\n",
    "            (1.0+purchaseScore)*searchScore AS score,\n",
    "            {{source: 'https://representative-domain/product/' + product.productCode}} AS metadata\n",
    "        ORDER BY purchaseScore DESC, searchScore DESC LIMIT 5\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# Use the same personalized recommendations as above but with a smaller limit\n",
    "def kg_recommendations_app(customer_id, k=30):\n",
    "    res = kg.query(\"\"\"\n",
    "    MATCH(:Customer {customerId:$customerId})-[:PURCHASED]->(:Article)\n",
    "    -[r:CUSTOMERS_ALSO_LIKE]->(:Article)-[:VARIANT_OF]->(product)\n",
    "    RETURN product.text + '\\nurl: ' + 'https://representative-domain/product/' + product.productCode  AS text,\n",
    "        sum(r.score) AS recommenderScore\n",
    "    ORDER BY recommenderScore DESC LIMIT $k\n",
    "    \"\"\", params={'customerId': customer_id, 'k':k})\n",
    "\n",
    "    return \"\\n\\n\".join([d['text'] for d in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "i7sCt8roekeH",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "Now, let's define our prompts. We will combine two:\n",
    "1. A system prompt which, in this case, tells the LLM how to generate the message\n",
    "2. A human prompt that just wraps the customer search(es)/interest(s)\n",
    "\n",
    "This will allow us to pass the customer interest(s) to the retriever but then also to the LLM for additional context when drafting the message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUAROR6aekeI"
   },
   "outputs": [],
   "source": [
    "general_system_template = '''\n",
    "You are a personal assistant named Sally for a fashion, home, and beauty company called HRM.\n",
    "write an email to {customerName}, one of your customers, to promote and summarize products relevant for them given the current season / time of year: {timeOfYear} .\n",
    "Please only mention the products listed below. Do not come up with or add any new products to the list.\n",
    "Each product comes with an https `url` field. Make sure to provide that https url with descriptive name text in markdown for each product.\n",
    "\n",
    "---\n",
    "# Relevant Products:\n",
    "{searchProds}\n",
    "\n",
    "# Customer May Also Be Interested In the following\n",
    " (pick items from here that pair with the above products well for the current season / time of year: {timeOfYear}.\n",
    " prioritize those higher in the list if possible):\n",
    "{recProds}\n",
    "---\n",
    "'''\n",
    "general_user_template = \"{searchPrompt}\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "    HumanMessagePromptTemplate.from_template(general_user_template),\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "TgFbaUt6ekeI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Create a Chain\n",
    "\n",
    "Now let's put a chain together that will leverage the retrievers, prompts, and LLM model. This is where Langchain shines, putting RAG together in a simple way.\n",
    "\n",
    "In addition to the personalized search and recommendations context, we will allow for some other parameters.\n",
    "\n",
    "1. `timeOfYear`: The time of year as a date, season, month, etc. so the LLM can tailor the language appropriately.\n",
    "2. `customerName`: Ordinarily, this can be pulled from the DB, but it has been scrubbed to maintain anonymity, so we will provide our own name here.\n",
    "\n",
    "You can potentially add other creative parameters here to help the LLM write relevant messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUpih07QdyXr"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# helper function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# LLM chain\n",
    "def chain_gen(customer_id):\n",
    "    return ({'searchProds': (lambda x:x['searchPrompt']) | kg_personalized_search_gen(customer_id).as_retriever(search_kwargs={\"k\": 100}) | format_docs,\n",
    "             'recProds': (lambda x:customer_id) |  RunnableLambda(kg_recommendations_app),\n",
    "             'customerName': lambda x:x['customerName'],\n",
    "             'timeOfYear': lambda x:x['timeOfYear'],\n",
    "             \"searchPrompt\":  lambda x:x['searchPrompt']}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JjbUGH6WekeI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Example Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkBdqOVjekeI"
   },
   "outputs": [],
   "source": [
    "chain = chain_gen(CUSTOMER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jL6P3IoydyXr",
    "outputId": "c6260e6f-982a-4f74-b8a3-64c09476abae"
   },
   "outputs": [],
   "source": [
    "print(chain.invoke({'searchPrompt':search_prompt, 'customerName':'Alex Smith', 'timeOfYear':'Feb, 2024'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy3fKoC1E0CC"
   },
   "source": [
    "#### Inspecting the Prompt Sent to the LLM\n",
    "In the above run, the LLM should only be using results from our Neo4j database to populate recommendations. Run the below cell to see the final prompt that was sent to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7-yDDUaD6FD",
    "outputId": "686d5f56-2cb8-49bc-8e29-6eb47cb40762"
   },
   "outputs": [],
   "source": [
    "def format_final_prompt(x):\n",
    "   return f'''=== Prompt to send to LLM ===\n",
    "   {x.to_string()}\n",
    "   === End Prompt ===\n",
    "   '''\n",
    "\n",
    "def chain_print_prompt(customer_id):\n",
    "    return ({'searchProds': (lambda x:x['searchPrompt']) | kg_personalized_search_gen(customer_id).as_retriever(search_kwargs={\"k\": 100}) | format_docs,\n",
    "             'recProds': (lambda x:customer_id) |  RunnableLambda(kg_recommendations_app),\n",
    "             'customerName': lambda x:x['customerName'],\n",
    "             'timeOfYear': lambda x:x['timeOfYear'],\n",
    "             \"searchPrompt\":  lambda x:x['searchPrompt']}\n",
    "            | prompt\n",
    "            | format_final_prompt\n",
    "            | StrOutputParser())\n",
    "\n",
    "print( chain_print_prompt(CUSTOMER_ID)\\\n",
    "      .invoke({'searchPrompt':search_prompt, 'customerName':'Alex Smith', 'timeOfYear':'Feb, 2024'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8G_vdFviekeI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Feel free to experiment and try more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeOts3Q4ZACL"
   },
   "outputs": [],
   "source": [
    "print(chain.invoke({'searchPrompt':\"western boots\", 'customerName':'Alex Smith', 'timeOfYear':'Feb, 2024'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1IU_gedrekeI",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Demo App\n",
    "Now letâ€™s use the above tools to create a demo app with Gradio.  We will need to make a couple more functions, but otherwise easy to fire up from a Notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1F0ve3cekeI"
   },
   "outputs": [],
   "source": [
    "# Create a means to generate and cache chains...so we can quickly try different customer ids\n",
    "personalized_search_chain_cache = dict()\n",
    "def get_chain(customer_id):\n",
    "    if customer_id in personalized_search_chain_cache:\n",
    "        return personalized_search_chain_cache[customer_id]\n",
    "    chain = chain_gen(customer_id)\n",
    "    personalized_search_chain_cache[customer_id] = chain\n",
    "    return chain\n",
    "\n",
    "# create multiple demo examples to try\n",
    "examples = [\n",
    "    [\n",
    "        CUSTOMER_ID,\n",
    "        'Feb, 2024',\n",
    "        'Alex Smith',\n",
    "        'Oversized Sweaters'\n",
    "    ],\n",
    "    [\n",
    "        '819f4eab1fd76b932fd403ae9f427de8eb9c5b64411d763bb26b5c8c3c30f16f',\n",
    "        'Feb, 2024',\n",
    "        'Robin Fischer',\n",
    "        'Oversized Sweaters'\n",
    "    ],\n",
    "    [\n",
    "        '44b0898ecce6cc1268dfdb0f91e053db014b973f67e34ed8ae28211410910693',\n",
    "        'Feb, 2024',\n",
    "        'Chris Johnson',\n",
    "        'Oversized Sweaters'\n",
    "    ],\n",
    "    [\n",
    "        '819f4eab1fd76b932fd403ae9f427de8eb9c5b64411d763bb26b5c8c3c30f16f',\n",
    "        'Feb, 2024',\n",
    "        'Robin Fischer',\n",
    "        'denim jeans'\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XsBcFQLlekeI",
    "outputId": "974df8d3-1e34-44e2-e355-3c452f3844c6"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def message_generator(*x):\n",
    "    chain = get_chain(x[0])\n",
    "    return chain.invoke({'searchPrompt':x[3], 'customerName':x[2], 'timeOfYear': x[1]})\n",
    "\n",
    "customer_id = gr.Textbox(value=CUSTOMER_ID, label=\"Customer ID\")\n",
    "time_of_year = gr.Textbox(value=\"Feb, 2024\", label=\"Time Of Year\")\n",
    "search_prompt_txt = gr.Textbox(value='Oversized Sweaters', label=\"Customer Interests(s)\")\n",
    "customer_name = gr.Textbox(value='Alex Smith', label=\"Customer Name\")\n",
    "message_result = gr.Markdown( label=\"Message\")\n",
    "\n",
    "demo = gr.Interface(fn=message_generator,\n",
    "                    inputs=[customer_id, time_of_year, customer_name, search_prompt_txt],\n",
    "                    outputs=message_result,\n",
    "                    examples=examples,\n",
    "                    title=\"ðŸª„ Message Generator ðŸ¥³\")\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Tucjvc73ZACM",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## That's a Wrap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfODXiJUZACM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
